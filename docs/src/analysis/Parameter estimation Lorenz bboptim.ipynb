{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation\n",
    "\n",
    "Parameter estimation for ODE models is provided by the JuliaDiffEq suite. The current functionality includes build_loss_objective. Note that these require that the problem be defined using a [ParameterizedFunction](https://github.com/JuliaDiffEq/ParameterizedFunctions.jl):\n",
    "\n",
    "\n",
    "## Setting up the parameter estimation problem\n",
    "\n",
    "Let's try parameter estimation for an ODE on the challenging Lorenz attractor. We base our implementation on the code from Paulo Marques available at: https://github.com/pjpmarques/Julia-Modeling-the-World/blob/master/Lorenz%20Attractor.ipynb and on the global optimization method provided by BlackBoxOptim.\n",
    "\n",
    "The system is formally described by three different differential equations. These equations represent the movement\n",
    "of a point $(x, y, z)$ in space over time. In the following equations, $t$ represents time, $\\sigma$, $\\rho$, $\\beta$ are constants.\n",
    "\n",
    "$$ \\begin{align}\n",
    "    \\frac{dx}{dt} &= \\sigma (y - x) \\\\\n",
    "    \\frac{dy}{dt} &= x (\\rho - z) - y \\\\\n",
    "    \\frac{dz}{dt} &= x y - bz\n",
    "\\end{align} $$\n",
    "\n",
    "\n",
    "Or in matrix form:\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "\\dot{x} \\\\\n",
    "\\dot{y} \\\\\n",
    "\\dot{z}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "- \\sigma & \\sigma & 0 \\\\\n",
    "r - z & -1 & - x \\\\\n",
    "y & x & -b\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial values of the system in space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True parameters: sigma, rho and beta are used to construct the dataset. In the estimation problem of the ODE system, these are unknown and should be estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using BlackBoxOptim.Parameters in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using DifferentialEquations, RecursiveArrayTools\n",
    "using BlackBoxOptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffEqBase.ODEProblem{Array{Float64,1},Float64,true,LorenzExample,DiffEqBase.CallbackSet{Tuple{},Tuple{}}}(LorenzExample,[0.1,0.0,0.0],(0.0,3.0),DiffEqBase.CallbackSet{Tuple{},Tuple{}}((),()))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = @ode_def_nohes LorenzExample begin\n",
    "  dx = σ*(y-x)\n",
    "  dy = x*(ρ-z) - y\n",
    "  dz = x*y - β*z\n",
    "end σ=>10.0 ρ=>28.0 β=>2.6666\n",
    "\n",
    "r0 = [0.1; 0.0; 0.0]\n",
    "tspan = (0.0, 4.0)\n",
    "prob = ODEProblem(g1, r0, tspan)\n",
    "tspan2 = (0.0, 3.0)\n",
    "prob_short = ODEProblem(g1, r0, tspan2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second phase of parameter estimation consists in forming the dataset by integrating the dynamic system over a time/space interval. \n",
    "\n",
    "Define the time vector and the interval grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4001-element Array{Float64,1}:\n",
       " 0.0  \n",
       " 0.001\n",
       " 0.002\n",
       " 0.003\n",
       " 0.004\n",
       " 0.005\n",
       " 0.006\n",
       " 0.007\n",
       " 0.008\n",
       " 0.009\n",
       " 0.01 \n",
       " 0.011\n",
       " 0.012\n",
       " ⋮    \n",
       " 3.989\n",
       " 3.99 \n",
       " 3.991\n",
       " 3.992\n",
       " 3.993\n",
       " 3.994\n",
       " 3.995\n",
       " 3.996\n",
       " 3.997\n",
       " 3.998\n",
       " 3.999\n",
       " 4.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = 0.001\n",
    "tf = 4.0\n",
    "tinterval = 0:dt:tf\n",
    "t  = collect(tinterval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in order to compare to the parameter estimation in the [Xiang2015] paper, we also use their integration interval of 300 observations instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301-element Array{Float64,1}:\n",
       " 0.0 \n",
       " 0.01\n",
       " 0.02\n",
       " 0.03\n",
       " 0.04\n",
       " 0.05\n",
       " 0.06\n",
       " 0.07\n",
       " 0.08\n",
       " 0.09\n",
       " 0.1 \n",
       " 0.11\n",
       " 0.12\n",
       " ⋮   \n",
       " 2.89\n",
       " 2.9 \n",
       " 2.91\n",
       " 2.92\n",
       " 2.93\n",
       " 2.94\n",
       " 2.95\n",
       " 2.96\n",
       " 2.97\n",
       " 2.98\n",
       " 2.99\n",
       " 3.0 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 0.01\n",
    "M = 300\n",
    "tstart = 0.0\n",
    "tstop = tstart + M * h\n",
    "tinterval_short = 0:h:tstop\n",
    "t_short = collect(tinterval_short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrate the ODE system from a starting point and into the future given a sequence of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#solve(prob_short,Euler(),tstops=t_short)\n",
    "#data_short = vecarr_to_arr(solve(prob,Euler(),tstops=t_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching vecarr_to_arr(::DiffEqBase.ODESolution{Array{Array{Float64,1},1},Void,Void,Array{Float64,1},Array{Array{Array{Float64,1},1},1},DiffEqBase.ODEProblem{Array{Float64,1},Float64,true,LorenzExample,DiffEqBase.CallbackSet{Tuple{},Tuple{}}},OrdinaryDiffEq.Vern7,OrdinaryDiffEq.InterpolationData{LorenzExample,Array{Array{Float64,1},1},Array{Float64,1},Array{Array{Array{Float64,1},1},1},OrdinaryDiffEq.Vern7Cache{Array{Float64,1},Array{Float64,1},Array{Float64,1},Array{Float64,1},OrdinaryDiffEq.Vern7ConstantCache{Float64,Float64}}}})\u001b[0m\nClosest candidates are:\n  vecarr_to_arr(\u001b[1m\u001b[31m::RecursiveArrayTools.AbstractVectorOfArray{T,N}\u001b[0m) at C:\\Users\\Denis\\.julia\\v0.5\\RecursiveArrayTools\\src\\vector_of_array.jl:44\u001b[0m",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching vecarr_to_arr(::DiffEqBase.ODESolution{Array{Array{Float64,1},1},Void,Void,Array{Float64,1},Array{Array{Array{Float64,1},1},1},DiffEqBase.ODEProblem{Array{Float64,1},Float64,true,LorenzExample,DiffEqBase.CallbackSet{Tuple{},Tuple{}}},OrdinaryDiffEq.Vern7,OrdinaryDiffEq.InterpolationData{LorenzExample,Array{Array{Float64,1},1},Array{Float64,1},Array{Array{Array{Float64,1},1},1},OrdinaryDiffEq.Vern7Cache{Array{Float64,1},Array{Float64,1},Array{Float64,1},Array{Float64,1},OrdinaryDiffEq.Vern7ConstantCache{Float64,Float64}}}})\u001b[0m\nClosest candidates are:\n  vecarr_to_arr(\u001b[1m\u001b[31m::RecursiveArrayTools.AbstractVectorOfArray{T,N}\u001b[0m) at C:\\Users\\Denis\\.julia\\v0.5\\RecursiveArrayTools\\src\\vector_of_array.jl:44\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "sol = solve(prob_short,Vern7(),saveat=t_short,abstol=1e-12,reltol=1e-12)\n",
    "data_short = vecarr_to_arr(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the actual/original state vectors that will be used for parameter estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#solve(prob,Euler(),tstops=t)\n",
    "#data = vecarr_to_arr(solve(prob,Euler(),tstops=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sol = solve(prob,Vern7(),saveat=t,abstol=1e-12,reltol=1e-12)\n",
    "data = vecarr_to_arr(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental layout of the data matrix is: equations as rows and sequence of observations as columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Non Linear Programming as an estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third phase of the parameter estimation problem is to set up the objective function of the NLP problem. The solution below should then be matched with build_loss_objective to simplify and standardize the NLP problem. The aim of build_loss_objective is to make the NLP problem compatible with solvers like Optim.jl and MathProgBase-associated solvers like NLopt.\n",
    "\n",
    "We have n observations $(\\boldsymbol{x}_i, y_i), i = 1,2...,n$, from a fixed-regressor nonlinear model with a known functional relationship $f$. Thus\n",
    "\n",
    "$$ \\begin{align}\n",
    "        y_i = f(\\boldsymbol{x}_i; \\boldsymbol{\\theta}^*) + \\epsilon_i & \\quad (i = 1,2,....,n)\n",
    "\\end{align} $$\n",
    "\n",
    "where $E[\\epsilon_i]$ = 0, $\\boldsymbol{x}_i$ is a k x 1 vector, and the true value $\\boldsymbol{\\theta}^*$ of $\\boldsymbol{\\theta}$ is known to belong to $\\boldsymbol{\\Theta}$, a subset of $R^p$. The nonlinear least squares estimate of $\\boldsymbol{\\theta}^*$, denoted by $\\boldsymbol{\\hat{\\theta}}$, minimizes the error sum of squares:\n",
    "\n",
    "$$ \\begin{align}\n",
    "S(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} [y_i - f(\\boldsymbol{x}_i;\\boldsymbol{\\theta})]^2 & \\quad   \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}.\n",
    "\\end{align} $$\n",
    "\n",
    "With the notation $f_i(\\boldsymbol{\\theta})$ = $f(\\boldsymbol{x}_i;\\boldsymbol{\\theta})$ and $\\boldsymbol{f}(\\boldsymbol{\\theta})$ = $\\big($$f_1(\\boldsymbol{\\theta})$, $f_2(\\boldsymbol{\\theta})$,$\\dots$, $f_n(\\boldsymbol{\\theta})$$\\big)$', the error sum of squares, $S(\\boldsymbol{\\theta})$, can also be re-written as:\n",
    "\n",
    "$$ \\begin{align}\n",
    "S(\\boldsymbol{\\theta}) = [y - \\boldsymbol{f}(\\boldsymbol{\\theta})]'[y - \\boldsymbol{f}(\\boldsymbol{\\theta})] = \\|y - \\boldsymbol{f}(\\boldsymbol{\\theta}\\|^2\n",
    "\\end{align} $$\n",
    "\n",
    "This quantity, labelled the data misfit function, is a quadratic metric, squared Euclidean distance,  named ESS = Error Sum of Squares, columnwise. This is the $S(\\boldsymbol{\\theta})$ function 2.2 and 2.7 of Seber and Wild page 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build cost function\n",
    "function ess2(actual, estimated)\n",
    "    sumsq = 0.0\n",
    "    for i in 1:size(actual, 2)\n",
    "        @inbounds sumsq += sumabs2(actual[i] .- estimated[:, i])\n",
    "    end\n",
    "    sumsq\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_cost_short(sol) = ess2(sol,data_short)\n",
    "my_cost(sol) = ess2(sol,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical problem is to find the value of $\\boldsymbol{\\theta}$ that minimizes a data misfit/loss function $\\boldsymbol{h(\\theta)}$ having one of the following forms:\n",
    "\n",
    "$$ \\begin{align}\n",
    "h(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} [y_i - f(\\boldsymbol{x}_i;\\boldsymbol{\\theta})]^2 = & \\sum_{i=1}^{n} r_i(\\boldsymbol{\\theta})^2,\n",
    "\\end{align} $$\n",
    "\n",
    "$$ \\begin{align}\n",
    "h(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} |y_i - f(\\boldsymbol{x}_i;\\boldsymbol{\\theta})| = & \\sum_{i=1}^{n} |r_i(\\boldsymbol{\\theta})|,\n",
    "\\end{align} $$\n",
    "\n",
    "$$ \\begin{align}\n",
    "h(\\boldsymbol{\\theta}) = \\sum_{i=1}^{n} \\rho(r_i(\\boldsymbol{\\theta})) & \\qquad \\text{(robust loss functions)}.\n",
    "\\end{align} $$\n",
    "\n",
    "These functions are available from two Julia packages: Distances.jl and LossFunctions.jl on Github. The ess function has been specified from first principles to check the accuracy and speed of alternative functions from other packages.\n",
    "\n",
    "We restrict attention to unconstrained minimization where we wish to minimize $h(\\boldsymbol{\\theta})$ over $\\mathbb{R}^p$. Most estimation methods require a global minimum of $h(\\boldsymbol{\\theta})$, namely a point $\\boldsymbol{\\hat{\\theta}}$ such that $h(\\boldsymbol{\\theta}) \\geq h(\\boldsymbol{\\hat{\\theta}})$ for all $\\boldsymbol{\\theta}$ in $\\mathbb{R}^p$. Global minimization is possible only for very restrictive clases of functions such as convex functions (Dixon and Szeg$\\ddot{o}$ [1978]. Therefore, the properties of the selected data misfit functions should check that they are, first and foremost, convex and subsidiarily smooth and/or continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Least Squares estimation (nls)\n",
    "\n",
    "A family of NLS estimators covers several cases of generalized or weighted least squares estimators. The main steps leading to the computation of $\\boldsymbol{\\hat{\\theta}}_G$, the value of $\\boldsymbol{\\theta}$ minimizing:\n",
    "\n",
    "$$ \\begin{align}\n",
    "S(\\boldsymbol{\\theta}) = [y - \\boldsymbol{f}(\\boldsymbol{\\theta})]'\\boldsymbol{V^{-1}}[y - \\boldsymbol{f}(\\boldsymbol{\\theta})],\n",
    "\\end{align} $$\n",
    "\n",
    "$\\boldsymbol{V}$ is a known positive definite matrix, $\\boldsymbol{J}$ is the Jacobian of $\\boldsymbol{f}(\\boldsymbol{\\theta})$ and $\\mathscr{\\hat{D}}[\\boldsymbol{\\hat{\\theta}}_G] = \\hat{\\sigma}^2(\\boldsymbol{\\hat{J}}'\\boldsymbol{V}^{-1}\\boldsymbol{\\hat{J}})^{-1}$           , an estimate of the asymptotic covariance matrix of $\\boldsymbol{\\hat{\\theta}}_G$:\n",
    "1. Perform a Cholesky decomposition of $\\boldsymbol{V} = \\boldsymbol{U}'\\boldsymbol{U}$ of $\\boldsymbol{V}$\n",
    "2. Solve $U'z = y$ and $U'k(\\boldsymbol{\\theta})$ for $\\boldsymbol{z}$ and $\\boldsymbol{k}$\n",
    "3. Apply an ordinary nonlinear least-squares technique to\n",
    "\n",
    "$$ \\begin{align}\n",
    "[z - \\boldsymbol{k}(\\boldsymbol{\\theta})]'[z - \\boldsymbol{k}(\\boldsymbol{\\theta})],\n",
    "\\end{align} $$\n",
    "\n",
    "Here we illustrate the simplest case when $V = I_n$ and the above steps collapse to an OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum-likelihood estimation (mle)\n",
    "\n",
    "As in the NLS case, the family of likelihood estimators covers several cases: maximum likelihood estimator, concentrated likelihood estimator and quasi-likelihood estimator.\n",
    "\n",
    "If the joint distribution of the $\\epsilon_i$ in the above model is assumed to be a known variable instead of a constant, then the maximum-likelihood estimate of $\\boldsymbol{\\theta}$ is obtained by maximizing the likelihood function. For $\\epsilon_i$ i.i.d $N(0, \\sigma^2)$ and ignoring some constants then the logarithm of the likelihood $L(\\boldsymbol{\\theta}, \\sigma^2)$ is:\n",
    "\n",
    "$$L(\\boldsymbol{\\theta}, \\sigma^2) = - \\frac{n}{2} ln  \\sigma^2 - \\frac{1}{2\\sigma^2}S(\\boldsymbol{\\theta})$$\n",
    "\n",
    "This function is maximized with respect to $\\boldsymbol{\\theta}$ when $S(\\boldsymbol{\\theta})$ is minimized when $\\boldsymbol{\\theta} = \\boldsymbol{\\hat{\\theta}}$, the least squares estimate.\n",
    "\n",
    "By analogy to the family of least squares estimators above, it is envisaged that this family of likelihood estimators should be coded as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the additional cases: the concentrated likelihood and the quasi-likelihood estimators. Each case depends on the properties of the covariance matrix of experimental noise.\n",
    "\n",
    "The concentrated likelihood method is a two-step method where the first step is the simple likelihood method explained above. It is documented on page 36 of Seber and Wild.\n",
    "\n",
    "The quasi-likelihood method is a further refinement by relaxing the underlying assumptions of MLE. It is documented on page 43 of Seber and Wild. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global optimization using BlackBoxOptim\n",
    "\n",
    "The parameter estimation of the Lorenz atractor has proved inaccurate with the family of algorithms available in NLopt. Instead, we are attempting the estimation of these parameters within the class of Differential Evolution (DE) algorithm available with BlackBoxOptim (https://github.com/robertfeldt/BlackBoxOptim.jl). BlackBoxOptim does not yet comply with MathProgBase syntax and the optimization has been set up by Robert Feldt below. It will become MathProgBase compliant in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the estimation capabilities of BlackBoxOptim, let's sample a small subset first and set the estimation bounds of the parameters as in the [Xiang2015] paper, https://www.hindawi.com/journals/ddns/2015/740721/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xiang2015Bounds = Tuple{Float64, Float64}[(9, 11), (20, 30), (2, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj_short = build_loss_objective(prob_short,Euler(),my_cost_short,tstops=t_short,dense=false)\n",
    "res1 = bboptimize(obj_short;SearchRange = Xiang2015Bounds, MaxSteps = 11e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = build_loss_objective(prob,Euler(),my_cost,tstops=t,dense=false)\n",
    "res1 = bboptimize(obj;SearchRange = Xiang2015Bounds, MaxSteps = 8e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's also try relaxing the tight bounds inthe Xiang2015 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LooserBounds = Tuple{Float64, Float64}[(0, 22), (0, 60), (1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = bboptimize(params -> nls_estimation(params, datastates1_short, times1_short); \n",
    "    SearchRange = LooserBounds, MaxSteps = 11e3) # They allow 12k fitness evals for 3-param estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Results from the short data sequence, 300 observations, used in [Xiang2015] paper:\")\n",
    "estfitness = nls_estimation(best_candidate(res2), datastates_short, t_short)\n",
    "@show (estfitness, best_candidate(res2), best_fitness(res1))\n",
    "datafitness = nls_estimation(true_params, datastates_short, t_short)\n",
    "@show (datafitness, true_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Results from the long data sequence from Paulo Marques:\")\n",
    "estfitness = nls_estimation(best_candidate(res1), datastates, t)\n",
    "@show (estfitness, best_candidate(res1), best_fitness(res2))\n",
    "datafitness = nls_estimation(true_params, datastates, t)\n",
    "@show (datafitness, true_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Results from the short data sequence used in [Xiang2015] paper, but with looser bounds:\")\n",
    "estfitness = nls_estimation(best_candidate(res3), datastates_short, t_short)\n",
    "@show (estfitness, best_candidate(res3), best_fitness(res3))\n",
    "datafitness = nls_estimation(true_params, datastates_short, t_short)\n",
    "@show (datafitness, true_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
